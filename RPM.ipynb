{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Práctica #1: Modelo para predicción de lluvia**\n",
    "\n",
    "*Centro Universitario de Ciencias Exactas e Ingenierías*\n",
    "\n",
    "*División de Tecnologías para la Integración Ciber-Humana*\n",
    "\n",
    "*Ingeniería Biomédica*\n",
    "\n",
    "<br>\n",
    "\n",
    "*Mtra. Sofía Alejandra Aguilar Valdez*\n",
    "\n",
    "2 de septiembre de 2022"
   ],
   "metadata": {
    "id": "nVx-kou4pjFk",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Información del equipo**\n",
    "\n",
    "```NOMBRES:``` \\\\\n",
    "Victor Alvarado Aparicio \\\\\n",
    "Dayana Analy Pacheco Bañuelos \\\\\n",
    "Leonardo Juàrez Zucco\n",
    "\n",
    "```CÓDIGOS:``` \\\\\n",
    "215767685 \\\\\n",
    "217535226 \\\\\n",
    "220285869\n",
    "\n",
    "```LINK REPOSITORIO:``` https://github.com/L-Rittmeister/U1_Team.git\n",
    "\n"
   ],
   "metadata": {
    "id": "dQNzG9Wm4ZQJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Contenido**\n",
    "\n",
    "\n",
    "\n",
    "1.   Resumen\n",
    "2.   Marco teórico\n",
    "3.   Objetivos\n",
    "4.   Materiales y métodos\n",
    "5.   Resultados\n",
    "6.   Discusión\n",
    "7.   Conclusiones\n",
    "8.   Referencias\n",
    "\n"
   ],
   "metadata": {
    "id": "QQ_tQMMJpude",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **1. Resumen**\n",
    "---\n",
    "En este trabajo se pretendía realizar un modelo de predicción de la lluvia utilizando datos de la estación meteorológica del aeropuerto de Seattle, para este modelo teorizamos utilizar 4 entradas de información para las 4 variables con las que contábamos que eran la probabilidad de precipitación, las temperaturas máxima y mínima y una confirmación si llovió o no con una capa oculta. La metodología usada empieza con la descarga datos, después la construcción del modelo con las entradas y variables mencionadas antes, además declaramos la función de costo nn.CrossEntropyLoss(), y también un algoritmo de optimización optim.Adam(model.parameters(), lr=0.001), para el ajuste de parámetros de la red, por último el entrenamiento del modelo y la evaluación de este. Al concluir la parte de la evaluación y prueba de la red, fue satisfactorio obtener el resultado fue el esperado.\n",
    "\n"
   ],
   "metadata": {
    "id": "anvzyOk06GH5",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **2. Marco teórico**\n",
    "---\n",
    "El modelo de predicción de la lluvia se basa en la información de la estación meteorológica del aeropuerto de Seattle, Washington, Estados Unidos, la cual se encuentra en el nivel del mar, con una latitud de 47.44° N y una longitud de 122.30° W.\n",
    "Para poder realizar un modelo funcional de una red neuronal es importante empezar con la descarga de los datos, para este caso se utilizó la librería pandas, la cual nos permite leer los datos de un archivo csv, en este caso se utilizó el archivo weatherHistory.csv. Luego de esto se procedió a la limpieza de los datos, para esto se eliminaron las columnas que no se utilizarían en el modelo, como la fecha, la descripción del clima y la precipitación acumulada. Después de esto se procedió a la construcción del modelo, para esto se utilizó la librería torch, la cual nos permite crear una red neuronal, en este caso se utilizó una red neuronal de 4 capas, la primera capa es la capa de entrada, la segunda capa es la capa oculta, la tercera capa es la capa de salida y la cuarta capa es la capa de activación. Después de esto se procedió a la definición de la función de costo, para esto se utilizó la librería torch.nn, la cual nos permite definir la función de costo, en este caso se utilizó la función de costo nn.CrossEntropyLoss(), la cual nos permite calcular la pérdida de la red neuronal. Después de esto se procedió a la definición del algoritmo de optimización, para esto se utilizó la librería torch.optim, la cual nos permite definir el algoritmo de optimización, en este caso se utilizó el algoritmo de optimización optim.Adam(model.parameters(), lr=0.001), el cual nos permite ajustar los parámetros de la red neuronal. Después de esto se procedió al entrenamiento del modelo, para esto se utilizó la librería torch, la cual nos permite entrenar la red neuronal, en este caso se utilizó el método train(), el cual nos permite entrenar la red neuronal. Después de esto se procedió a la evaluación del modelo, para esto se utilizó la librería torch, la cual nos permite evaluar la red neuronal, en este caso se utilizó el método test(), el cual nos permite evaluar la red neuronal. Al finalizar la evaluación del modelo se procedió a la prueba del modelo, para esto se utilizó la librería torch, la cual nos permite probar la red neuronal, en este caso se utilizó el método predict(). Al finalizar la prueba del modelo se obtuvo un resultado satisfactorio, el cual nos permite concluir que el modelo es funcional.\n"
   ],
   "metadata": {
    "id": "E8r4C9H26UTK",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **3. Objetivos**\n",
    "---\n",
    "\n",
    "### Procesar los datos y crear los objetos e instancias necesarios para poder hacer correr un modelo de predicción basado en una red neuronal que sea capaz de predecir si lloverá o no en Seattle, Washington, USA, utilizando los datos de la estación meteorológica del aeropuerto de Seattle.\n",
    "### Entrenar el modelo con los datos provistos por la fuente dada y probarlo con los datos de prueba. Evaluar el modelo y considerar un posible replanteamiento si se presentan fenómenos como underfitting u overfitting, así como problemas de compatibilidad de datos, arquitectura del modelo e implementación de las funciones de entrenamiento y validación.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **4. Materiales y métodos**\n",
    "---\n",
    "---\n",
    "## *Materiales*\n",
    "---\n",
    "Describir este conjunto de datos [[1]](https://www.kaggle.com/code/fatmakursun/rain-forecasting-with-artificial-neural-network/data).\n",
    "\n",
    "## *Métodos*\n",
    "---\n",
    "### 4.1. Esquema de metodología\n",
    "---"
   ],
   "metadata": {
    "id": "4rX1w4ZD6mPj",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Mounting drive storage to access data (if needed/running on collab then uncomment)\n",
    "# Mount Google Drive\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q4vifyfcnWi8",
    "outputId": "efaa1931-50e4-43e0-bd3e-8753f70d9868",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split, DataLoader"
   ],
   "metadata": {
    "id": "_0Fnd_SX-hHi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Pytorch and CUDA installation test\n",
    "torch.randn(5).cuda()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qHX258IhurIP",
    "outputId": "eba9a781-6c0f-4301-9924-4d2c36c53af4",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "             DATE  PRCP  TMAX  TMIN   RAIN\n0      1948-01-01  0.47    51    42   True\n1      1948-01-02  0.59    45    36   True\n2      1948-01-03  0.42    45    35   True\n3      1948-01-04  0.31    45    34   True\n4      1948-01-05  0.17    45    32   True\n...           ...   ...   ...   ...    ...\n25546  2017-12-10  0.00    49    34  False\n25547  2017-12-11  0.00    49    29  False\n25548  2017-12-12  0.00    46    32  False\n25549  2017-12-13  0.00    48    34  False\n25550  2017-12-14  0.00    50    36  False\n\n[25551 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DATE</th>\n      <th>PRCP</th>\n      <th>TMAX</th>\n      <th>TMIN</th>\n      <th>RAIN</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1948-01-01</td>\n      <td>0.47</td>\n      <td>51</td>\n      <td>42</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1948-01-02</td>\n      <td>0.59</td>\n      <td>45</td>\n      <td>36</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1948-01-03</td>\n      <td>0.42</td>\n      <td>45</td>\n      <td>35</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1948-01-04</td>\n      <td>0.31</td>\n      <td>45</td>\n      <td>34</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1948-01-05</td>\n      <td>0.17</td>\n      <td>45</td>\n      <td>32</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>25546</th>\n      <td>2017-12-10</td>\n      <td>0.00</td>\n      <td>49</td>\n      <td>34</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>25547</th>\n      <td>2017-12-11</td>\n      <td>0.00</td>\n      <td>49</td>\n      <td>29</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>25548</th>\n      <td>2017-12-12</td>\n      <td>0.00</td>\n      <td>46</td>\n      <td>32</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>25549</th>\n      <td>2017-12-13</td>\n      <td>0.00</td>\n      <td>48</td>\n      <td>34</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>25550</th>\n      <td>2017-12-14</td>\n      <td>0.00</td>\n      <td>50</td>\n      <td>36</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>25551 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the dataset on a pandas dataframe\n",
    "DataF = pd.read_csv(\"seattleWeather_1948-2017.csv\")\n",
    "DataF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#Drop NaN value\n",
    "DataF.dropna(inplace=True)\n",
    "DataF.reset_index(inplace=True, drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "#eliminate Date column from the dataset\n",
    "DataF.drop(['DATE'], axis=1, inplace=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "#Transform the data to float\n",
    "DataF['PRCP'] = DataF['PRCP'].astype(float)\n",
    "DataF['TMAX'] = DataF['TMAX'].astype(float)\n",
    "DataF['TMIN'] = DataF['TMIN'].astype(float)\n",
    "DataF['RAIN'] = DataF['RAIN'].astype(float)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "#Convert the dataframe to a numpy array\n",
    "DataF = DataF.to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "#Create the dataset class using the PRCP, TMAX, TMIN as inputs and RAIN as label\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.len = data.shape[0]\n",
    "        self.x_data = torch.from_numpy(self.data[:, :3])\n",
    "        self.y_data = torch.from_numpy(self.data[:, 3])\n",
    "        self.y_data = self.y_data.type(torch.LongTensor)\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "#Create the train and test dataset using the Dataset class and the data from the dataframe with 90% of the data for training and 10% for testing\n",
    "train_data = Dataset(DataF[:int(0.9*len(DataF))])\n",
    "test_data = Dataset(DataF[int(0.9*len(DataF)):])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "#Create the train and test dataloader using the train and test dataset\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "#Create the model class using 3 inputs and 2 hidden layers with 3 neurons each and 1 output\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_inputs):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden1 = nn.Linear(n_inputs, 3)\n",
    "        self.hidden2 = nn.Linear(3, 3)\n",
    "        self.hidden3 = nn.Linear(3, 3)\n",
    "        self.output = nn.Linear(3, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = self.relu(self.hidden2(x))\n",
    "        x = self.relu(self.hidden3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "#Create the model using the model class and the number of inputs\n",
    "model = Model(3).double().cuda()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "#Create the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "#Define train function\n",
    "def train(model, train_loader, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for batch_idx, (data, target) in tqdm(enumerate(train_loader)):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "#Define test function\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/22993 (0%)]\tLoss: 0.705724\n",
      "Train Epoch: 0 [6400/22993 (28%)]\tLoss: 0.592912\n",
      "Train Epoch: 0 [12800/22993 (56%)]\tLoss: 0.655360\n",
      "Train Epoch: 0 [19200/22993 (83%)]\tLoss: 0.565280\n",
      "Train Epoch: 1 [0/22993 (0%)]\tLoss: 0.499678\n",
      "Train Epoch: 1 [6400/22993 (28%)]\tLoss: 0.516660\n",
      "Train Epoch: 1 [12800/22993 (56%)]\tLoss: 0.591309\n",
      "Train Epoch: 1 [19200/22993 (83%)]\tLoss: 0.468335\n",
      "Train Epoch: 2 [0/22993 (0%)]\tLoss: 0.454736\n",
      "Train Epoch: 2 [6400/22993 (28%)]\tLoss: 0.398781\n",
      "Train Epoch: 2 [12800/22993 (56%)]\tLoss: 0.576110\n",
      "Train Epoch: 2 [19200/22993 (83%)]\tLoss: 0.517679\n",
      "Train Epoch: 3 [0/22993 (0%)]\tLoss: 0.411307\n",
      "Train Epoch: 3 [6400/22993 (28%)]\tLoss: 0.500073\n",
      "Train Epoch: 3 [12800/22993 (56%)]\tLoss: 0.542112\n",
      "Train Epoch: 3 [19200/22993 (83%)]\tLoss: 0.448598\n",
      "Train Epoch: 4 [0/22993 (0%)]\tLoss: 0.508184\n",
      "Train Epoch: 4 [6400/22993 (28%)]\tLoss: 0.435625\n",
      "Train Epoch: 4 [12800/22993 (56%)]\tLoss: 0.425460\n",
      "Train Epoch: 4 [19200/22993 (83%)]\tLoss: 0.455429\n",
      "Train Epoch: 5 [0/22993 (0%)]\tLoss: 0.443879\n",
      "Train Epoch: 5 [6400/22993 (28%)]\tLoss: 0.545635\n",
      "Train Epoch: 5 [12800/22993 (56%)]\tLoss: 0.518909\n",
      "Train Epoch: 5 [19200/22993 (83%)]\tLoss: 0.588853\n",
      "Train Epoch: 6 [0/22993 (0%)]\tLoss: 0.443546\n",
      "Train Epoch: 6 [6400/22993 (28%)]\tLoss: 0.375603\n",
      "Train Epoch: 6 [12800/22993 (56%)]\tLoss: 0.402705\n",
      "Train Epoch: 6 [19200/22993 (83%)]\tLoss: 0.501745\n",
      "Train Epoch: 7 [0/22993 (0%)]\tLoss: 0.342405\n",
      "Train Epoch: 7 [6400/22993 (28%)]\tLoss: 0.492830\n",
      "Train Epoch: 7 [12800/22993 (56%)]\tLoss: 0.452619\n",
      "Train Epoch: 7 [19200/22993 (83%)]\tLoss: 0.432922\n",
      "Train Epoch: 8 [0/22993 (0%)]\tLoss: 0.397314\n",
      "Train Epoch: 8 [6400/22993 (28%)]\tLoss: 0.442839\n",
      "Train Epoch: 8 [12800/22993 (56%)]\tLoss: 0.296187\n",
      "Train Epoch: 8 [19200/22993 (83%)]\tLoss: 0.565099\n",
      "Train Epoch: 9 [0/22993 (0%)]\tLoss: 0.422926\n",
      "Train Epoch: 9 [6400/22993 (28%)]\tLoss: 0.524038\n",
      "Train Epoch: 9 [12800/22993 (56%)]\tLoss: 0.431197\n",
      "Train Epoch: 9 [19200/22993 (83%)]\tLoss: 0.473324\n",
      "Train Epoch: 10 [0/22993 (0%)]\tLoss: 0.419156\n",
      "Train Epoch: 10 [6400/22993 (28%)]\tLoss: 0.359922\n",
      "Train Epoch: 10 [12800/22993 (56%)]\tLoss: 0.395703\n",
      "Train Epoch: 10 [19200/22993 (83%)]\tLoss: 0.482556\n",
      "Train Epoch: 11 [0/22993 (0%)]\tLoss: 0.399310\n",
      "Train Epoch: 11 [6400/22993 (28%)]\tLoss: 0.337161\n",
      "Train Epoch: 11 [12800/22993 (56%)]\tLoss: 0.422670\n",
      "Train Epoch: 11 [19200/22993 (83%)]\tLoss: 0.422114\n",
      "Train Epoch: 12 [0/22993 (0%)]\tLoss: 0.429682\n",
      "Train Epoch: 12 [6400/22993 (28%)]\tLoss: 0.407511\n",
      "Train Epoch: 12 [12800/22993 (56%)]\tLoss: 0.265424\n",
      "Train Epoch: 12 [19200/22993 (83%)]\tLoss: 0.371446\n",
      "Train Epoch: 13 [0/22993 (0%)]\tLoss: 0.399345\n",
      "Train Epoch: 13 [6400/22993 (28%)]\tLoss: 0.326522\n",
      "Train Epoch: 13 [12800/22993 (56%)]\tLoss: 0.368359\n",
      "Train Epoch: 13 [19200/22993 (83%)]\tLoss: 0.392401\n",
      "Train Epoch: 14 [0/22993 (0%)]\tLoss: 0.331533\n",
      "Train Epoch: 14 [6400/22993 (28%)]\tLoss: 0.317358\n",
      "Train Epoch: 14 [12800/22993 (56%)]\tLoss: 0.277296\n",
      "Train Epoch: 14 [19200/22993 (83%)]\tLoss: 0.330694\n",
      "Train Epoch: 15 [0/22993 (0%)]\tLoss: 0.290324\n",
      "Train Epoch: 15 [6400/22993 (28%)]\tLoss: 0.368840\n",
      "Train Epoch: 15 [12800/22993 (56%)]\tLoss: 0.340537\n",
      "Train Epoch: 15 [19200/22993 (83%)]\tLoss: 0.337164\n",
      "Train Epoch: 16 [0/22993 (0%)]\tLoss: 0.241129\n",
      "Train Epoch: 16 [6400/22993 (28%)]\tLoss: 0.407282\n",
      "Train Epoch: 16 [12800/22993 (56%)]\tLoss: 0.238368\n",
      "Train Epoch: 16 [19200/22993 (83%)]\tLoss: 0.292029\n",
      "Train Epoch: 17 [0/22993 (0%)]\tLoss: 0.325818\n",
      "Train Epoch: 17 [6400/22993 (28%)]\tLoss: 0.301915\n",
      "Train Epoch: 17 [12800/22993 (56%)]\tLoss: 0.317606\n",
      "Train Epoch: 17 [19200/22993 (83%)]\tLoss: 0.376036\n",
      "Train Epoch: 18 [0/22993 (0%)]\tLoss: 0.258858\n",
      "Train Epoch: 18 [6400/22993 (28%)]\tLoss: 0.282875\n",
      "Train Epoch: 18 [12800/22993 (56%)]\tLoss: 0.331963\n",
      "Train Epoch: 18 [19200/22993 (83%)]\tLoss: 0.212951\n",
      "Train Epoch: 19 [0/22993 (0%)]\tLoss: 0.294222\n",
      "Train Epoch: 19 [6400/22993 (28%)]\tLoss: 0.412901\n",
      "Train Epoch: 19 [12800/22993 (56%)]\tLoss: 0.312385\n",
      "Train Epoch: 19 [19200/22993 (83%)]\tLoss: 0.225001\n",
      "Train Epoch: 20 [0/22993 (0%)]\tLoss: 0.272188\n",
      "Train Epoch: 20 [6400/22993 (28%)]\tLoss: 0.332080\n",
      "Train Epoch: 20 [12800/22993 (56%)]\tLoss: 0.213347\n",
      "Train Epoch: 20 [19200/22993 (83%)]\tLoss: 0.213959\n",
      "Train Epoch: 21 [0/22993 (0%)]\tLoss: 0.271999\n",
      "Train Epoch: 21 [6400/22993 (28%)]\tLoss: 0.385968\n",
      "Train Epoch: 21 [12800/22993 (56%)]\tLoss: 0.235441\n",
      "Train Epoch: 21 [19200/22993 (83%)]\tLoss: 0.261467\n",
      "Train Epoch: 22 [0/22993 (0%)]\tLoss: 0.253160\n",
      "Train Epoch: 22 [6400/22993 (28%)]\tLoss: 0.323844\n",
      "Train Epoch: 22 [12800/22993 (56%)]\tLoss: 0.183123\n",
      "Train Epoch: 22 [19200/22993 (83%)]\tLoss: 0.199824\n",
      "Train Epoch: 23 [0/22993 (0%)]\tLoss: 0.263478\n",
      "Train Epoch: 23 [6400/22993 (28%)]\tLoss: 0.278562\n",
      "Train Epoch: 23 [12800/22993 (56%)]\tLoss: 0.256751\n",
      "Train Epoch: 23 [19200/22993 (83%)]\tLoss: 0.270445\n",
      "Train Epoch: 24 [0/22993 (0%)]\tLoss: 0.261130\n",
      "Train Epoch: 24 [6400/22993 (28%)]\tLoss: 0.292391\n",
      "Train Epoch: 24 [12800/22993 (56%)]\tLoss: 0.275808\n",
      "Train Epoch: 24 [19200/22993 (83%)]\tLoss: 0.139635\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "train(model, train_loader, criterion, optimizer, 25)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0032, Accuracy: 2379/2555 (93%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Test the model\n",
    "test(model, test_loader, criterion)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Use the model to predict the rain\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        print(pred)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "#Save the model\n",
    "torch.save(model.state_dict(), \"model.pth\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Load the model\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### 4.2 Descripción de los métoddos y su implementación en código en forma de narrativa\n",
    "---\n",
    "Las librerias principales utilizadas fueron en primer lugar Pandas para la manipulación de los datos,Numpy para las transformaciones matematicas así como la construcción de los objetos matematicos para poder , luego Pytorch para la creación del modelo y la implementación de la red neuronal,\n",
    "Para asegurarnos de que PyTorch haya sido importado correctamente creamos un tensor con 5 números aleatorios dentro de una distribución normal y lo cargamos a CUDA, una arquitectura unificada que permite usar las unidades de procesamiento de gráficos para procesamiento de propósitos generales.\n",
    "Enseguida creamos un dataframe de pandas al que le cargamos la lectura de la base de datos en CSV que contiene los datos del clima en Seatle de 1948 – 2017, y lo mostramos.\n",
    "Dentro del dataframe eliminamos las filas que contienen datos “NaN” y reiniciamos los índices evitando que se añadan como una columna más al dataframe.\n",
    "Después, borramos los datos con el header ‘DATE’ en el eje vertical dentro del mismo dataframe.\n",
    "Convertimos todos los datos del dataframe en numeros decimales flotantes, para después convertir todo el dataframe en un arreglo numérico de numpy.\n",
    "Creamos una clase llamada “Dataset” de tipo dataset. Definimos su método de inicialización con el parámetro data. El atributo data será igual al parámetro data ingresado. Guardamos el tamaño del eje horizontal de la data ingresada en el atributo “len”. Creamos un tensor de un arreglo de numpy con la data ingresada hasta la columna 3 (usamos columnas 0, 1 y 2) y lo guardamos en el atributo “x_data”. Creamos otro tensor de un arreglo de numpy con la data ingresada en la columna 3 y lo guardamos en el atributo “y_data”, mismo que convertimos a un tensor de tipo long (para datos enteros de hasta 64 bits, negativos y positivos). Definimos también un método llamado “getitem” al cual ingresa un índice y devuelve el valor de x_data y y_data en el índice especificado. Por último, creamos un método “len” que devuelve el valor de “len” obtenido en el método de inicialización.\n",
    "Creamos dos datasets, uno de training que utilice la data del arreglo numérico de numpy hasta el 90% de su tamaño, redondeando a un número entero, y uno de testing que utilice el 10% restante redondeado a un número entero.\n",
    "Utilizando los datasets de training y testing, creamos los dataloaders respectivos con un batch_size de 64 y abilitamos el shuffle para revolver los datos.\n",
    "Creamos una clase llamada “Model” del tipo nn.Module. Definimos su método de inicialización con el parámetro n_inputs y heredamos los atributos de la clase superior. Aplicamos una transformación lineal utilizando el valor de n_inputs como tamaño de las muestras de entrada y especificamos 3 muestras de salida y guardamos el valor en “hidden1”. Replicamos el proceso dos veces más utilizando ahora 3 para el tamaño de muestras de entrada y de salida, guardando los valores en “hidden2” y “hidden3” respectivamente. Aplicamos una transformación lineal más, pero esta vez cambiamos el tamaño de muestras de salida a 2, para crear nuestra capa de salida, y guardamos su valor en “output”. Por último, en este método, asignamos la función de unidad lineal rectificada al atributo “relu” para utilizarla como función de activación. Definimos después otro método llamado “forward” con el parámetro x. En este guardaremos el valor de x resultante del uso de la función de activación en las tres capas ocultas, y de su paso por la capa de salida. Devolvemos x como salida del modelo.\n",
    "Creamos un objeto de la clase “Model” con un parámetro de n_inputs = 3 que trabaje con números decimales flotantes y lo cargamos a CUDA. Creamos también un objeto que contiene la función de costo, y un objeto que contiene el algoritmo de optimización “Adam”, con los parámetros del modelo y un learning rate de 0.001, como tensor."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **5. Resultados**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **6. Discusión**\n",
    "---\n",
    "Inicialmente el programa tenia problemas en la parte del entrenamiento y la evaluación creemos que fue por algún algoritmo mal planteado en la red o incluso alguna función mal planteada. Sin embargo, al revisar minuciosamente encontramos que habíamos incluido los labels en los datos de entrada. Procedimos a reacomodar nuestros datos, ahora en forma de datasets para que fuéramos capaces de generar los tensores de data y labels. Como convertimos nuestros datos a flotantes, tuvimos que adaptar el modelo para funcionar con el mismo tipo de dato. Decidimos utilizar un optimizador diferente y separar las funciones e entrenamiento y validación de los ciclos respectivos para probar otro método y mantener el modelo lo más versátil posible entro de nuestro entendimiento.\n",
    "El modelo fue probado utilizando 10, 20, 25, 30, 50 y 100 épocas, cada una dando resultados diferentes. Los resultados al correrse con 10 épocas mostraban una precisión baja (56%). Decidimos entonces probar lo que consideramos un exceso de épocas, por los datos que procesamos. Con 100 épocas obtuvimos una precisión del 100%, lo que nos llevó a desconfiar del modelo, ya que podría indicar memorización y no aprendizaje. A partir de estos resultados iniciales de underfitting y overfitting decidimos aproximarnos a un valor que indicara aprendizaje, poca pérdida y alta precisión. 25 épocas nos brindó estas características, por lo que lo consideramos la medida correcta para el entrenamiento de nuestro modelo. Cabe destacar, que cada entrenamiento y testeo se llevaron a cabo con modelos limpios para evitar errores y mantener la integridad de los procesos.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **7. Conclusiones**\n",
    "---\n",
    "En el salón de clases sentí que la creación de un modelo era una tarea sencilla. Me di cuenta de que en realidad lo es, siempre y cuando se tenga una fuerte base de conocimiento derivado del análisis del problema que queremos resolver y las metodologías implementadas en el estado del arte, conocimiento con el que yo no contaba. Analizar los errores, buscar soluciones, aprender de las librerías y su documentación es un proceso que lleva su tiempo y requiere de cierta disposición al replanteamiento de las ideas y conceptos previos para construir nuevos. Sé bien que de este punto en adelante me encontraré con estos conflictos una y otra vez, por lo que entrenar mi plasticidad de adaptabilidad de ideas será un reto diario, no obstante, me llevará a poder crear buenos cimientos para la proposición y construcción de proyectos con soluciones viables por métodos de deep learning.\n",
    "\n",
    "Al finalizar el modelo de predicción de la lluvia no se cumplieron los objetivos tal cual se esperaba inicialmente, sin embargo, con algo de esfuerzo se obtuvo lo esperado, fue un reto trabajar con este tipo de ANN en múltiples capas y la red neuronal, además para aterrizar lo aprendido y aprender a usar git hub, me pareció una experiencia bien.\n",
    "\n",
    "En particular Este trabajo demoestró una complejidad y un desafío en la implementación de redes neuronales en la plataforma pytorch y si bien los resultados no fueron satisfactorios fue una buena experiencia para aprender a utilizar esta plataforma y sus herramientas."
   ],
   "metadata": {
    "id": "8Gb2ac0VSryX",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **8. Referencias**\n",
    "\n",
    "[1] Pb, V. (2020, February 18). Perceptron. Kaggle. Retrieved June 1, 2022, from https://www.kaggle.com/code/prashfio/perceptron/notebook"
   ],
   "metadata": {
    "id": "cxRqAiW5sG_m",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}